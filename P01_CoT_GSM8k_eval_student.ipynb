{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc26d17",
   "metadata": {},
   "source": [
    "Chain-of-Thought Reasoning Evaluation on GSM8K Dataset\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b9fde",
   "metadata": {},
   "source": [
    "**Created by Dr Chao Shu (chao.shu@qmul.ac.uk)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54417e8c",
   "metadata": {},
   "source": [
    "Submitted By\n",
    "\n",
    "**Name**:\n",
    "\n",
    "**QMUL ID**:\n",
    "\n",
    "**BUPT ID**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5425e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803bd8b",
   "metadata": {},
   "source": [
    "Chain-of-Thought (CoT) is the fundamental technique behind cutting-edge reasoning Large Language Models (LLMs). This notebook guides you through implementing and evaluating different CoT reasoning approaches on the GSM8K dataset. \n",
    "\n",
    "You will implement Zero-shot CoT, Few-shot CoT, Self-consistency CoT as well as a standard Input-Ouput prompting as the baseline. You will evaluate their effectiveness by solving math word problems in the GSM8K dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377e21c",
   "metadata": {},
   "source": [
    "## Prerequisite: Ollama and Qwen2 1.5B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9621fb",
   "metadata": {},
   "source": [
    "Before you start this project, you should already have ollama and Qwen2 1.5B model installed on your computer and be able to run \"qwen2:1.5b\" model locally on your computer since we have finished L01 and you should be able to run all the examples in L01.\n",
    "\n",
    "If you haven't done so, this guide will help you set up Ollama and download the qwen2:1.5b model required for this notebook.\n",
    "\n",
    "**1. Install Ollama**\n",
    "\n",
    "Please go to [Ollama GitHub repository](https://github.com/ollama/ollama) or [webpage](https://ollama.com/) and follow the instructions to install Ollama.\n",
    "\n",
    "**2. Download the qwen2:1.5b Model**\n",
    "\n",
    "After installing Ollama, open a terminal (Command Prompt on Windows) and run:\n",
    "\n",
    "```bash\n",
    "ollama pull qwen2:1.5b\n",
    "```\n",
    "\n",
    "This will download the model, which may take some time depending on your internet connection.\n",
    "\n",
    "**3. Verify Installation**\n",
    "\n",
    "To verify that everything is working correctly, run:\n",
    "\n",
    "```bash\n",
    "ollama list\n",
    "```\n",
    "\n",
    "You should see `qwen2:1.5b` in the list of available models.\n",
    "\n",
    "**4. Troubleshooting**\n",
    "\n",
    "- If you encounter permission issues on Linux or macOS, try running commands with `sudo`\n",
    "- If the model download fails, check your internet connection and try again\n",
    "- For more help, visit the [Ollama GitHub repository](https://github.com/ollama/ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83252df0",
   "metadata": {},
   "source": [
    "# PART I: Implement Functions for the Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0883d0f",
   "metadata": {},
   "source": [
    "In this part, you will build the foundation for evaluating different reasoning approaches by implementing key functions that will:\n",
    "\n",
    "1. **Set up the model**: Configure and prepare the language model for evaluation\n",
    "2. **Process the dataset**: Load and parse the GSM8K math problem dataset\n",
    "3. **Implement CoT prompting strategies**: Create prompt templates for different reasoning approaches\n",
    "4. **Extract and evaluate answers**: Parse model outputs and compare with ground truth\n",
    "5. **Implement Self-Consistency CoT**: Generate multiple reasoning paths and use majority voting\n",
    "\n",
    "For each key function, you will complete a practical implementation and then test it with simplified examples to ensure everything works correctly. This careful testing is crucial as any errors in these foundational functions would impact your final evaluation results. Please also carefully check the output reasoning results from the LLM to get familar with reasoning behaviours of the LLM and make sure the required functions are implemented correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023b418",
   "metadata": {},
   "source": [
    "To get started, let's import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15869873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from collections import Counter\n",
    "from IPython.display import Markdown, display\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898ba00",
   "metadata": {},
   "source": [
    "## Function to Set up an LLM Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b766015",
   "metadata": {},
   "source": [
    "First, let's implement a function to set up an LLM that is used to evaluate different CoT prompting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b4cd444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model_name=\"qwen2:1.5b\", temperature=0.7, top_p=0.95):\n",
    "    \"\"\"Set up the LLM using LangChain\"\"\"\n",
    "    \n",
    "    # Ollama platform\n",
    "    print(f\"Setting up Ollama model: {model_name}, temperature: {temperature}, top_p: {top_p}\")\n",
    "    return ChatOllama(\n",
    "        model=model_name,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        num_predict=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6a3b6",
   "metadata": {},
   "source": [
    "### Test the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468cfc6",
   "metadata": {},
   "source": [
    "**[Q1.1]** Test the function following the TODO instructions. Fix any error if there is any. **[5 Marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1091c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Setup an LLM using Qwen2 1.5B model with default parameters, temperature=0.7 and top_p=0.95. \n",
    "# Save the model in a variable called `test_llm`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4bb48",
   "metadata": {},
   "source": [
    "## Functions for GSM8K Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade9e59",
   "metadata": {},
   "source": [
    "The GSM8K dataset consists of high-quality grade school math word problems. Let's implement functions to load the GSM8K dataset and extract numerical answers from the provided ground-truth solutions.\n",
    "- By default, the function will create a directory called \"*data*\" in the same directory of this notebook to save the dataset downloaded from Hugging Face.\n",
    "- The numerical answer of each question is placed behind \"####\" string in the \"answer\" text. You can find the format of dataset after you run the test codes at the end of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc68c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gsm8k_dataset(split=\"test\", sample_size=None, random_seed=42, cache_dir=\"./data\"):\n",
    "    \"\"\"\n",
    "    Load the GSM8K dataset from the Hugging Face datasets library\n",
    "    \n",
    "    Args:\n",
    "        split: Which dataset split to load (\"train\" or \"test\")\n",
    "        sample_size: Optional limit on number of examples to load\n",
    "        random_seed: Seed for random sampling (for reproducibility)\n",
    "        cache_dir: Directory to cache the downloaded dataset\n",
    "    \n",
    "    Returns:\n",
    "        List of examples from the GSM8K dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = load_dataset(\"gsm8k\", \"main\", cache_dir=cache_dir)\n",
    "        examples = dataset[split]\n",
    "        \n",
    "        if sample_size and sample_size < len(examples):\n",
    "            # Generate random indices for sampling\n",
    "            np.random.seed(random_seed)\n",
    "            random_indices = np.random.choice(\n",
    "                range(len(examples)), \n",
    "                size=sample_size, \n",
    "                replace=False  # Sample without replacement\n",
    "            )\n",
    "            # Use random indices to select examples\n",
    "            examples = examples.select(random_indices)\n",
    "            print(f\"Randomly selected {sample_size} examples with seed {random_seed}\")\n",
    "        \n",
    "        return examples\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GSM8K dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_ground_truth_answer(example):\n",
    "    \"\"\"\n",
    "    Extract ground truth answer from a GSM8K example\n",
    "    \n",
    "    Args:\n",
    "        example: A single example from the GSM8K dataset\n",
    "    \n",
    "    Returns:\n",
    "        The ground truth answer as a string\n",
    "    \"\"\"\n",
    "    # GSM8K answers are typically in the format \"#### X\"\n",
    "    answer_match = re.search(r'####\\s*([\\d\\.\\-]+)', example[\"answer\"])\n",
    "    if answer_match:\n",
    "        return answer_match.group(1).strip()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad5990",
   "metadata": {},
   "source": [
    "### Test the Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d418d787",
   "metadata": {},
   "source": [
    "**[Q1.2]** Test the function following the TODO instructions. Fix any error if there is any. **[5 Marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a0a7a",
   "metadata": {},
   "source": [
    "We'll randomly select **10** problems from the **\"test\"** split of the GSM8K dataset using your student ID as the randome seed and explore the first problem in the randomly selected problems as a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af76c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !!IMPORTANT!! Set your Student ID. Save your student ID (e.g., '221154321') to a variable named \"STUDENT_ID\"\n",
    "STUDENT_ID = \"221154321\"  # Replace with your actual student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b931494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert student ID to integer for use as random seed\n",
    "try:\n",
    "    STUDENT_SEED = int(STUDENT_ID)\n",
    "    print(f\"Using student ID {STUDENT_ID} as random seed value {STUDENT_SEED}\")\n",
    "    # TODO: Randomly select 10 problems in the \"test\" split using the student ID as seed. Save the problems in a variable called `test_problems`.\n",
    "    test_problems = \"Your codes here\"\n",
    "\n",
    "    print(f\"Selected {len(test_problems)} problems using student ID as random seed\")\n",
    "    print(f\"Example Question: {test_problems[0]['question']}\")\n",
    "    print(f\"Example Question's answer: {test_problems[0]['answer']}\")\n",
    "    print(f\"Extracted answer: {get_ground_truth_answer(test_problems[0])}\")\n",
    "except ValueError:\n",
    "    print(f\"Invalid Student ID {STUDENT_SEED}. Please enter a valid Student ID in previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3a45f",
   "metadata": {},
   "source": [
    "## Implement CoT Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a7bcbf",
   "metadata": {},
   "source": [
    "Let's set prompts for Zero-Shot CoT, Few-Shot CoT as well as a standard prompt (Input-Output) as the baseline for our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7921f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New ChatPromptTemplates with simpler format\n",
    "STANDARD_CHAT_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the questions briefly with just a few words without reasoning..\"\n",
    "              \"Put your final numerical answer within $\\\\boxed{{}}$\"),\n",
    "    (\"human\", \"Question: {question}\")\n",
    "])\n",
    "\n",
    "# Zero-shot Chain of Thought Chat Prompt\n",
    "ZERO_SHOT_COT_CHAT_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the questions by thinking step by step. For each question, carefully break down your reasoning process, provide each logical step, followed by a numerical answer. Put your final numerical answer within \\\\boxed{{}}. \"\n",
    "              \"The response structure should be: \\n\\nThinking: [your reasoning steps] \\n Answer: $\\\\boxed{{}}$.\"),\n",
    "    (\"human\", \"Question: {question}\")\n",
    "])\n",
    "\n",
    "# Few-shot Chain of Thought Chat Prompt\n",
    "FEW_SHOT_COT_CHAT_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the questions by thinking step by step. For each question, carefully break down your reasoning process, provide each logical step, followed by a numerical answer. Put your final numerical answer within \\\\boxed{{}}. \"\n",
    "              \"A few examples will be provided for you to follow and understand how to think step by step. \"\n",
    "              \"The response structure should be: \\n\\nThinking: [your reasoning steps] \\n Answer: $\\\\boxed{{}}$.\"),\n",
    "    (\"human\", \"\"\"\n",
    "Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "Thinking: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6.\n",
    "Answer: $\\\\boxed{{6}}$\n",
    "\n",
    "Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "Thinking: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.\n",
    "Answer: $\\\\boxed{{5}}$\n",
    "\n",
    "Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "Thinking: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39.\n",
    "Answer: $\\\\boxed{{39}}$\n",
    "\n",
    "Question: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
    "Thinking: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8.\n",
    "Answer: $\\\\boxed{{8}}$\n",
    "\n",
    "Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
    "Thinking: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9.\n",
    "Answer: $\\\\boxed{{9}}$\n",
    "\n",
    "Question: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
    "Thinking: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29.\n",
    "Answer: $\\\\boxed{{29}}$\n",
    "\n",
    "Question: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
    "Thinking: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls.\n",
    "Answer: $\\\\boxed{{33}}$\n",
    "\n",
    "Question: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "Thinking: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8.\n",
    "Answer: $\\\\boxed{{8}}$\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e051762",
   "metadata": {},
   "source": [
    "### Test the Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab863c",
   "metadata": {},
   "source": [
    "**[Q1.3]** Please use the first test problem (`test_problems[0]`) you obtained from the previous section and the `ChatPromptTemplate.invoke()` method to test the 3 prompts. Fix any error if there is any. **[5 Marks]**\n",
    "\n",
    "> 💡 Note:\n",
    "> Please check if the `content` of the `SystemMessage` and the `HumanMessage` meet the expectation as defined in `STANDARD_CHAT_PROMPT`, `ZERO_SHOT_COT_CHAT_PROMPT` and `FEW_SHOT_COT_CHAT_PROMPT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e9a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the standard chat prompt with the first problem in the test_problems data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a4dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the Zero-Shot CoT chat prompt with the first problem in the test_problems data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df106620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the Few-Shot CoT chat prompt with the first problem in the test_problems data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a599b5fd",
   "metadata": {},
   "source": [
    "## Utility Functions for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89331d42",
   "metadata": {},
   "source": [
    "Let's implement utility functions for parsing model responses and evaluating the correctness of answers.\n",
    "- `extract_response_answer()`: A general function used to extract the numerical answer from a reasoning LLM's response.\n",
    "- `evaluate_predictions()`: Compare the numercal answers from an LLM to the ground truth and calculate accuracy (solve rate) for one round of evaluation (all problems in the test dataset).\n",
    "- `evaluate_predictions()`: Calculate summary statistics from multiple rounds of evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c318e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response_answer(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the final numerical answer from the model's response.\n",
    "    \n",
    "    Args:\n",
    "        response: The full text response from the LLM\n",
    "    \n",
    "    Returns:\n",
    "        The extracted numerical answer as a string\n",
    "    \"\"\"\n",
    "    # First check for answer tags: <answer>X</answer>\n",
    "    answer_tag_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)\n",
    "    if answer_tag_match:\n",
    "        answer_content = answer_tag_match.group(1).strip()\n",
    "        # Extract number from answer content if it contains text\n",
    "        numbers = re.findall(r'[-+]?\\d*\\.?\\d+', answer_content.replace(',', ''))\n",
    "        if numbers:\n",
    "            return numbers[-1].strip()\n",
    "        return answer_content.strip()\n",
    "    \n",
    "    # Then try to find boxed answer format: \\boxed{X}\n",
    "    boxed_match = re.search(r'\\\\boxed{([^{}]+)}', response)\n",
    "    if boxed_match:\n",
    "        box_content = boxed_match.group(1).strip()\n",
    "        # Extract number from box content if it contains text\n",
    "        numbers = re.findall(r'[-+]?\\d*\\.?\\d+', box_content.replace(',', ''))\n",
    "        if numbers:\n",
    "            return numbers[-1].strip()\n",
    "        return box_content.strip()\n",
    "    \n",
    "    # If no boxed answer, try to find \"Answer: X\" pattern\n",
    "    answer_match = re.search(r'Answer:\\s*([\\d\\.\\-]+)', response)\n",
    "    if answer_match:\n",
    "        return answer_match.group(1).strip()\n",
    "    \n",
    "    # Otherwise try to find the last number in the text\n",
    "    numbers = re.findall(r'(\\d+\\.?\\d*|\\.\\d+)', response)\n",
    "    if numbers:\n",
    "        return numbers[-1].strip()\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def evaluate_predictions(predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the model's predictions against reference answers\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of predicted answers\n",
    "        references: List of reference answers\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    if len(predictions) != len(references):\n",
    "        raise ValueError(\"Predictions and references must have the same length\")\n",
    "    \n",
    "    correct = 0\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        try:\n",
    "            # Clean and convert to float for numerical comparison\n",
    "            pred_value = float(pred.replace(',', '').strip())\n",
    "            ref_value = float(ref.replace(',', '').strip())\n",
    "            if pred_value == ref_value:\n",
    "                correct += 1\n",
    "        except (ValueError, TypeError):\n",
    "            # If conversion fails, it's wrong\n",
    "            pass\n",
    "    \n",
    "    accuracy = correct / len(references) if references else 0\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"correct\": correct,\n",
    "        \"total\": len(references)\n",
    "    }\n",
    "\n",
    "def calculate_statistics(accuracies):\n",
    "    \"\"\"Calculate summary statistics from multiple rounds of evaluation\"\"\"\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_dev = np.std(accuracies, ddof=1)  # Sample standard deviation\n",
    "    min_accuracy = np.min(accuracies)\n",
    "    max_accuracy = np.max(accuracies)\n",
    "    range_accuracy = max_accuracy - min_accuracy\n",
    "    \n",
    "    return {\n",
    "        \"mean\": float(mean_accuracy),\n",
    "        \"std_dev\": float(std_dev),\n",
    "        \"min\": float(min_accuracy),\n",
    "        \"max\": float(max_accuracy),\n",
    "        \"range\": float(range_accuracy),\n",
    "        \"all_values\": [float(acc) for acc in accuracies]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03bb69",
   "metadata": {},
   "source": [
    "## Functions for Different CoT Prompting Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f06b70",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's implement the different prompting strategies for Chain-of-Thought reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b27f1d",
   "metadata": {},
   "source": [
    "### Standard, Zero-Shot and Few-Shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf0646",
   "metadata": {},
   "source": [
    "**[Q1.4]** Please implement the function a single problem using the provided prompt template and LLM (SC-CoT not included) by completing the `TODO` parts in the function.  **[8 Marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to solve a single problem using the provided prompt template and LLM (SC-CoT not included)\n",
    "def solve_single_problem(problem, prompt_template, llm):\n",
    "    \"\"\"\n",
    "    Solve a single problem using the provided prompt template and LLM\n",
    "    Args:\n",
    "        problem: A single example from the GSM8K dataset\n",
    "        prompt_template: The prompt template to use\n",
    "        llm: The LLM to use for generating responses\n",
    "    Returns:\n",
    "        A dictionary with the question, reference answer, model response, prediction, and correctness\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Extract question and reference answer (ground truth). Save them in variables called `question` and `reference`\n",
    "    question = \"Your codes here\"\n",
    "    reference = \"Your codes here\"\n",
    "    \n",
    "    # Skip examples where we can't extract a ground truth\n",
    "    if not reference:\n",
    "        return None\n",
    "    \n",
    "    # TODO: Create LCEL chain\n",
    "    # Your codes here\n",
    "    \n",
    "    # Use LCEL chain to get response\n",
    "    try:\n",
    "        # TODO: Invoke the chain with the question to get the response. Save the response in a variable called `response_text`\n",
    "        response_text = \"Your codes here\"\n",
    "        \n",
    "        # TODO: Extract answer from the LLM's response. Save the answer in a variable called `prediction`\n",
    "        prediction = \"Your codes here\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example: {e}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "    \n",
    "    # Create a dictionary to store the result\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"response\": response_text,\n",
    "        \"prediction\": prediction,\n",
    "        \"correct\": prediction.strip() == reference.strip()\n",
    "    }\n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd4136",
   "metadata": {},
   "source": [
    "#### Test the Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6c36e",
   "metadata": {},
   "source": [
    "**[Q1.5]** Let's test if we can get a response with \"Thinking\" and \"Answer\" parts from the `test_llm` for the `test_problems[0]` using **Few-Shot CoT** and extract the numerical answer from the response.  **[2 Marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ceb81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the `test_llm` to solve the `test_problems[0]` using the Few-Shot CoT chat prompt.\n",
    "# Save the result in a variable called `test_result`\n",
    "test_result = \"Your codes here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the result\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the response in a pretty format\n",
    "display(Markdown(test_result[\"response\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7599c",
   "metadata": {},
   "source": [
    "### SC-CoT based on Zero-Shot or Few-Shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2425de9",
   "metadata": {},
   "source": [
    "**[Q1.6]** Please implement the function to solve a single problem using SC-CoT reasoning technique by completing the `TODO` parts in the function. The SC-CoT can be based on either Zero-Shot or Few-Shot prompting (both should be supported). **[20 Marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f77df",
   "metadata": {},
   "source": [
    "As mentioned in Lesson 01 notebook, please implement the SC-CoT so that the response follows the format below, i.e., each reasoning path should start with \"path_i: \". An example of multi-path response (some parts are omitted due to the lengthy response):\n",
    "\n",
    "```text\n",
    "{'path_1': \"To find out how much each of the other two pizzas cost, let's break down what we know:\\n\\n1. Total cost for four pizzas: $64\\n2. Cost for two pizzas: $30\\n\\nLet's denote the price of one pizza as \\\\(x\\\\).\\n\\nSo, the equation representing the total cost would be:\\n\\\\[2x + 2x = 30\\\\]\\nSimplifying this gives us:\\n\\\\[4x = 30\\\\]\\n\\nTo find out how much each of the other two pizzas cost (\\\\(2x\\\\) since there are two), divide both sides by \\\\(2\\\\):\\n\\\\[x = \\\\frac{30}{2}\\\\]\\n\\\\[x = 15\\\\]\\n\\nSo, each of the other two pizzas costs $15.\", \n",
    "\n",
    "'path_2': \"To solve this problem, ... Therefore, each of the remaining two pizzas cost \\\\$17.\", \n",
    "\n",
    "'path_3': \"Let's break down the problem step-by-step.... Step 6: Present the final answer.\\n- Each of the other two pizzas costs $17.\\n\\nTherefore, the answer is $17.\", \n",
    "\n",
    "'path_4': ... Therefore, each of the other two pizzas cost $17.\", \n",
    "\n",
    "'path_5': 'To find out how much each of the other two pizzas cost ... Therefore, each of the two remaining pizzas cost $1.33.'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d4f00",
   "metadata": {},
   "source": [
    "The implementation of `solve_single_problem_sc_cot()` does not have to strictly follow the TODO instructions inside. You can develop your own implementation with the help from any AI tools. **However, you MUST obtain the required values of the specified variables to fill in the `result` dictionary as the return value**.\n",
    "\n",
    "> 💡 Note:\n",
    "> \n",
    "> You will get 10 marks IF the SC-CoT is correctly implemented (All fields in the `result` dictionary are correct, showing multiple reasoning paths and a correct majority voting answer)\n",
    "> \n",
    "> You wlll get 20 marks IF the SC-CoT is correctly implemented with parallel inference (without using the `for` loop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfc800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to solve a single problem using SC-CoT \n",
    "def solve_single_problem_sc_cot(problem, prompt_template, llm, num_paths=3, temperatures=[0.7, 0.7, 0.7]):\n",
    "    \"\"\"\n",
    "    Solve a single problem using SC-CoT based on the provided prompt template and LLM\n",
    "    Args:\n",
    "        problem: A single example from the GSM8K dataset\n",
    "        prompt_template: The prompt template to use\n",
    "        llm: The LLM to use for generating responses\n",
    "        num_paths: Number of reasoning paths to generate for each question\n",
    "    Returns:\n",
    "        A dictionary with the question, reference answer, model response, prediction, and correctness\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Extract question and reference answer (ground truth). Save them in variables called `question` and `reference`\n",
    "    question = \"Your codes here\"\n",
    "    reference = \"Your codes here\"\n",
    "    \n",
    "    # Skip examples where we can't extract a ground truth\n",
    "    if not reference:\n",
    "        return None\n",
    "\n",
    "    # Truncate if too many temperatures provided\n",
    "    # To support sampling with diverse temperatures. You do not have to use this variable in this project)\n",
    "    temperatures = temperatures[:num_paths] \n",
    "    \n",
    "    # TODO: Create SC-CoT chain (optional)\n",
    "    # Your codes here\n",
    "\n",
    "    # Generate multiple reasoning paths with different temperatures\n",
    "    try:\n",
    "        # TODO: Get all reasoning paths\n",
    "        paths = \"Your codes here\"\n",
    "        \n",
    "        # TODO: Extract answers from each path\n",
    "        path_answers = []\n",
    "        # Your codes here\n",
    "        \n",
    "        # TODO: Determine majority vote answer\n",
    "        if path_answers:\n",
    "            majority_answer = \"Your codes here\"\n",
    "            \n",
    "            # For debugging\n",
    "            # print(f\"Path answers: {path_answers}, Majority: {majority_answer}\")\n",
    "            \n",
    "            # TODO: Create consolidated response text showing all paths and the majority answer\n",
    "            consolidated_response = \"Your codes here\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example with SC-CoT: {e}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "                \n",
    "    # Create a dictionary to store the result\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"response\": consolidated_response,\n",
    "        \"paths\": paths,\n",
    "        \"path_answers\": path_answers,\n",
    "        \"prediction\": majority_answer,\n",
    "        \"correct\": majority_answer.strip() == reference.strip()\n",
    "    }\n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae585b63",
   "metadata": {},
   "source": [
    "#### Test the Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58738bf",
   "metadata": {},
   "source": [
    "**[Q1.7]** Let's test if we can get a response with **3** reasoning paths from the `test_llm` for the `test_problems[0]` based on **Zero-Shot CoT** prompting and correctly extract the majority answer from the multiple reasoning paths.  **[5 Marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a343f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the `test_llm` to solve the `test_problems[0]` using SC-CoT base on the Zero-Shot chat prompt.\n",
    "# Save the result in a variable called `test_result`\n",
    "num_paths = 3\n",
    "temperatures = [0.7] * num_paths\n",
    "test_result = \"Your codes here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98613ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the result\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the response in a pretty format\n",
    "display(Markdown(test_result[\"response\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453cc60",
   "metadata": {},
   "source": [
    "## Functions for CoT Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec154a",
   "metadata": {},
   "source": [
    "**[Q1.8]** Now let's implement the functions to evaluate different CoT strategies.  **[4 Marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(llm, problems, prompt_template, prompt_name, rounds=1,\n",
    "                   sc_cot_flag=False, num_paths=3, temperatures=[0.7, 0.7, 0.7]):\n",
    "    \"\"\"\n",
    "    Run evaluation with a specific prompting strategy for multiple rounds\n",
    "    \n",
    "    Args:\n",
    "        llm: The language model to use\n",
    "        problems: Dataset examples to evaluate\n",
    "        prompt_template: The prompt template to use\n",
    "        prompt_name: Name of the prompting strategy for logging\n",
    "        rounds: Number of evaluation rounds to run\n",
    "        num_paths: Number of reasoning paths to generate for each question\n",
    "        temperatures: List of temperature values to use for each path\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (summary_metrics, all_round_metrics, all_round_responses)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store results for all rounds\n",
    "    all_metrics = []\n",
    "    all_responses = []\n",
    "    all_accuracies = []\n",
    "    \n",
    "    if sc_cot_flag:\n",
    "        print(f\"Running {rounds} rounds of SC-CoT evaluation with {prompt_name}...\")\n",
    "    else:\n",
    "        print(f\"Running {rounds} rounds of evaluation with {prompt_name}...\")\n",
    "    \n",
    "    for round_num in range(1, rounds + 1):\n",
    "        # Initialize lists to store predictions and references for this round\n",
    "        predictions = []\n",
    "        references = []\n",
    "        round_responses = []\n",
    "        \n",
    "        if sc_cot_flag:\n",
    "            print(f\"Round {round_num}/{rounds} - {prompt_name} (Self-Consistency)\")\n",
    "        else:\n",
    "            print(f\"Round {round_num}/{rounds} - {prompt_name}\")\n",
    "        \n",
    "        for problem in tqdm(problems, desc=f\"Round {round_num}\"):\n",
    "            # TODO: Solve the problem using the LLM and specified CoT tenchique\n",
    "            # Your codes here\n",
    "            result = \"Your codes here\"  # This is only a placeholder, please replace it with your codes and add more lines if needed\n",
    "\n",
    "            # Skip if result is None (e.g., no ground truth)\n",
    "            if result is None:\n",
    "                continue\n",
    "\n",
    "            # Save the results\n",
    "            predictions.append(result[\"prediction\"])\n",
    "            references.append(result[\"reference\"])\n",
    "            round_responses.append(result)\n",
    "        \n",
    "        # Calculate metrics for this round\n",
    "        round_metrics = evaluate_predictions(predictions, references)\n",
    "        if sc_cot_flag:\n",
    "            print(f\"Round {round_num} - {prompt_name} SC-CoT Results: {round_metrics}\")\n",
    "        else:\n",
    "            print(f\"Round {round_num} - {prompt_name} Results: {round_metrics}\")\n",
    "        \n",
    "        # Store the round results\n",
    "        all_metrics.append({\n",
    "            \"round_number\": round_num,\n",
    "            \"metrics\": round_metrics\n",
    "        })\n",
    "        all_responses.append(round_responses)\n",
    "        all_accuracies.append(round_metrics[\"accuracy\"])\n",
    "        \n",
    "        # Print round results\n",
    "        print(f\"  Round {round_num}: {round_metrics['accuracy']:.2%}\")\n",
    "    \n",
    "    # Calculate summary statistics across all rounds\n",
    "    summary_stats = calculate_statistics(all_accuracies)\n",
    "    \n",
    "    return summary_stats, all_metrics, all_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b180f",
   "metadata": {},
   "source": [
    "### Test the Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e6e173",
   "metadata": {},
   "source": [
    "Test the function following the `TODO` instructions. Fix any error if there is any."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e41665",
   "metadata": {},
   "source": [
    "**[Q1.9]** Run the evaluation for standard prompt for 2 rounds using `test_llm` and the 10 problems in the `test_problems` data. Please set the value of the `prompt_name` argument when you call `run_evaluation()` to `\"Standard Prompt\"`.  **[3 Marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c10a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the standard prompt evaluation for 2 rounds\n",
    "print(\"\\nEvaluating Standard Prompt:\")\n",
    "# Your codes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inspect the summary statistics and metrics for the two round\n",
    "# Your codes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inspect the responses to make sure the results and the statistics are consistent for each round\n",
    "# Your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a83119",
   "metadata": {},
   "source": [
    "**[Q1.10]** Run the evaluation for few-shot SC-CoT with 3 reasoning paths and temperature 0.7 for 2 rounds using `test_llm` and the 10 problems in the `test_problems` data. Please set the value of the `prompt_name` argument when you call `run_evaluation()` to `\"Few-shot SC-CoT\"`.  **[3 Marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a91604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run few-shot SC-CoT evaluation with 3 reasoning paths and temperature 0.7 for 2 rounds\n",
    "print(\"\\nEvaluating Few-shot Self-Consistency CoT:\")\n",
    "# Your codes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inspect the summary statistics and metrics for the two round\n",
    "# Your codes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inspect the responses to make sure the results and the statistics are consistent for each round\n",
    "# Your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bfbee",
   "metadata": {},
   "source": [
    "# PART II: Run Evaluations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6285fa98",
   "metadata": {},
   "source": [
    "In this part, we'll assemble all components we've built and run the evaluation on 100 randomly selected GSM8K problems using all CoT methods. Let's find out how each CoT technique performs in comparison with others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddadf72",
   "metadata": {},
   "source": [
    "## Configure the Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db2e7f8",
   "metadata": {},
   "source": [
    "Let's configure the parameters for the evaluation.\n",
    "-  Please make sure you run the cell to set the `STUDENT_SEED` variable to your QM Student ID in the \"**Load GSM8K Dataset**\" section.\n",
    "-  Please do not change values for other parameters to obtain the results you are going to submit with the notebook. You can play with different configurations (e.g., using different models, different temperatures, number of rounds, number of paths for SC-CoT, etc.), for your own experiments if you are interested.\n",
    "-  We choose a relatively low temperature (0.25) to reduce the variation of the results since our sample size is small.\n",
    "-  Typically, the number of reasoning paths for SC-CoT is power of 2. We use 5 to save evaluation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bd2b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for evaluation\n",
    "eval_config = {\n",
    "    \"platform\": \"ollama\",\n",
    "    \"model\": \"qwen2:1.5b\",\n",
    "    \"temperature\": 0.25,\n",
    "    \"top_p\": 0.95,\n",
    "    \"sample_size\": 100,\n",
    "    \"random_seed\": STUDENT_SEED,\n",
    "    \"rounds\": 5,\n",
    "    \"num_sc_paths\": 5,\n",
    "    \"output_dir\": \"./output\"\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7781ac",
   "metadata": {},
   "source": [
    "## Set up an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58f3c5",
   "metadata": {},
   "source": [
    "**[Q2.1]** Please setup an LLM based on the evaluation configuration. Save the model in a variable called `llm`.  **[1 Mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6928ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Setup an LLM based on the evaluation configuration. Save the model in a variable called `llm`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b8036",
   "metadata": {},
   "source": [
    "## Load GSM8K Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ddfdc",
   "metadata": {},
   "source": [
    "**[Q2.2]** Please load the GSM8K dataset with the specified sample size and random seed in the `eval_config`. Save the problems in a variable called `eval_problems`.  **[1 Mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302aba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the GSM8K dataset with the specified sample size and random seed in the `eval_config`.\n",
    "# Save the problems in a variable called `eval_problems`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06612abd",
   "metadata": {},
   "source": [
    "## Run CoT Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e08dc",
   "metadata": {},
   "source": [
    "**[Q2.3]** Please complete the script below following the TODO instructions and run the evaluations for the standard, zero-shot and few-shot CoT prompts for the number of rounds specified in `eval_config`.  **[3 Marks]**\n",
    "\n",
    "Please set the value of the `prompt_name` argument when you call `run_evaluation()` to `\"Standard Prompt\"`, `Zero-shot CoT` and `Few-shot CoT`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7390f",
   "metadata": {},
   "source": [
    "> 💡 Note:\n",
    "> \n",
    "> This evaluation takes about 35 min to complete on a Macbook Pro M2 with 8GB RAM. Please allow enough time to run this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(eval_config[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# Initialize results structure\n",
    "all_results = {\n",
    "    \"config\": eval_config,\n",
    "    \"standard\": {},\n",
    "    \"zero_shot_cot\": {},\n",
    "    \"few_shot_cot\": {},\n",
    "    \"zero_shot_sc_cot\": {},\n",
    "    \"few_shot_sc_cot\": {}\n",
    "}\n",
    "\n",
    "# TODO: Run standard prompt evaluation\n",
    "print(\"\\nEvaluating Standard Prompt:\")\n",
    "standard_summary, standard_rounds, standard_responses = \"Your\", \"codes\", \"here\"\n",
    "all_results[\"standard\"] = {\n",
    "    \"summary\": standard_summary,\n",
    "    \"rounds\": standard_rounds\n",
    "}\n",
    "\n",
    "# TODO: Run zero-shot CoT evaluation\n",
    "print(\"\\nEvaluating Zero-shot CoT:\")\n",
    "zero_shot_summary, zero_shot_rounds, zero_shot_responses = \"Your\", \"codes\", \"here\"\n",
    "all_results[\"zero_shot_cot\"] = {\n",
    "    \"summary\": zero_shot_summary,\n",
    "    \"rounds\": zero_shot_rounds\n",
    "}\n",
    "\n",
    "# TODO: Run few-shot CoT evaluation\n",
    "print(\"\\nEvaluating Few-shot CoT:\")\n",
    "few_shot_summary, few_shot_rounds, few_shot_responses = \"Your\", \"codes\", \"here\"\n",
    "all_results[\"few_shot_cot\"] = {\n",
    "    \"summary\": few_shot_summary,\n",
    "    \"rounds\": few_shot_rounds\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7be456",
   "metadata": {},
   "source": [
    "**[Q2.4]** Please complete the script below following the TODO instructions and run the evaluations for the SC-CoT with zero-shot and few-shot CoT prompts for the number of rounds specified in `eval_config`.  **[2 Marks]**\n",
    "\n",
    "Please set the value of the `prompt_name` argument when you call `run_evaluation()` to `\"Zero-shot SC-CoT\"` and `\"Few-shot SC-CoT\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0aab36",
   "metadata": {},
   "source": [
    "> 💡 Note:\n",
    "> \n",
    "> This evaluation takes about 150 min to complete on a Macbook Pro M2 with 8GB RAM. Please allow enough time to run this evaluation. You can put the following two evaluations in two cells and run separately if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set temperatures for SC-CoT evaluations\n",
    "temperatures = [eval_config[\"temperature\"]] * eval_config[\"num_sc_paths\"]\n",
    "\n",
    "# TODO: Run zero-shot SC-CoT evaluation\n",
    "print(\"\\nEvaluating Zero-shot Self-Consistency CoT:\")\n",
    "zero_shot_sc_summary, zero_shot_sc_rounds, zero_shot_sc_responses = \"Your\", \"codes\", \"here\"\n",
    "all_results[\"zero_shot_sc_cot\"] = {\n",
    "    \"summary\": zero_shot_sc_summary,\n",
    "    \"rounds\": zero_shot_sc_rounds\n",
    "}\n",
    "\n",
    "# TODO: Run few-shot SC-CoT evaluation\n",
    "print(\"\\nEvaluating Few-shot Self-Consistency CoT:\")\n",
    "few_shot_sc_summary, few_shot_sc_rounds, few_shot_sc_responses = \"Your\", \"codes\", \"here\"\n",
    "all_results[\"few_shot_sc_cot\"] = {\n",
    "    \"summary\": few_shot_sc_summary,\n",
    "    \"rounds\": few_shot_sc_rounds\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56aa85a",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1619e8",
   "metadata": {},
   "source": [
    "**[Q2.5]** Save the metrics/statistics file and responses files for 5 the reasoning techniques.  **[18 Marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df51a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save consolidated metrics\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_identifier = f\"{eval_config['platform']}_{eval_config['model']}\".replace(\"/\", \"-\").replace(\":\", \"_\")\n",
    "metrics_file = f\"{eval_config['output_dir']}/metrics_{model_identifier}_{eval_config['rounds']}rounds_{timestamp}.json\"\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "# Save responses for each prompt type and round\n",
    "os.makedirs(f\"{eval_config['output_dir']}/responses\", exist_ok=True)\n",
    "\n",
    "for round_num, responses in enumerate(standard_responses, 1):\n",
    "    responses_file = f\"{eval_config['output_dir']}/responses/standard_{model_identifier}_round{round_num}_{timestamp}.json\"\n",
    "    with open(responses_file, \"w\") as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "        \n",
    "for round_num, responses in enumerate(zero_shot_responses, 1):\n",
    "    responses_file = f\"{eval_config['output_dir']}/responses/zero_shot_{model_identifier}_round{round_num}_{timestamp}.json\"\n",
    "    with open(responses_file, \"w\") as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "        \n",
    "for round_num, responses in enumerate(few_shot_responses, 1):\n",
    "    responses_file = f\"{eval_config['output_dir']}/responses/few_shot_{model_identifier}_round{round_num}_{timestamp}.json\"\n",
    "    with open(responses_file, \"w\") as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "        \n",
    "for round_num, responses in enumerate(zero_shot_sc_responses, 1):\n",
    "    responses_file = f\"{eval_config['output_dir']}/responses/zero_shot_sc_{model_identifier}_round{round_num}_{timestamp}.json\"\n",
    "    with open(responses_file, \"w\") as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "        \n",
    "for round_num, responses in enumerate(few_shot_sc_responses, 1):\n",
    "    responses_file = f\"{eval_config['output_dir']}/responses/few_shot_sc_{model_identifier}_round{round_num}_{timestamp}.json\"\n",
    "    with open(responses_file, \"w\") as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "\n",
    "print(f\"All results saved to {metrics_file}\")\n",
    "\n",
    "# Print final summary with statistics\n",
    "print(\"\\n===== EVALUATION SUMMARY =====\")\n",
    "print(f\"Platform: {eval_config['platform']}\")\n",
    "print(f\"Model: {eval_config['model']}\")\n",
    "print(f\"Temperature: {eval_config['temperature']}\")\n",
    "print(f\"Top-p: {eval_config['top_p']}\")\n",
    "print(f\"Sample size: {eval_config['sample_size']}\")\n",
    "print(f\"Rounds: {eval_config['rounds']}\")\n",
    "print(f\"SC-CoT paths: {eval_config['num_sc_paths']}\")\n",
    "\n",
    "print(\"\\nStandard Prompt:\")\n",
    "print(f\"  Mean: {standard_summary['mean']:.2%}\")\n",
    "print(f\"  Range: {standard_summary['min']:.2%} - {standard_summary['max']:.2%} (±{standard_summary['range']/2:.2%})\")\n",
    "print(f\"  Std Dev: {standard_summary['std_dev']:.4f}\")\n",
    "\n",
    "print(\"\\nZero-shot CoT:\")\n",
    "print(f\"  Mean: {zero_shot_summary['mean']:.2%}\")\n",
    "print(f\"  Range: {zero_shot_summary['min']:.2%} - {zero_shot_summary['max']:.2%} (±{zero_shot_summary['range']/2:.2%})\")\n",
    "print(f\"  Std Dev: {zero_shot_summary['std_dev']:.4f}\")\n",
    "\n",
    "print(\"\\nFew-shot CoT:\")\n",
    "print(f\"  Mean: {few_shot_summary['mean']:.2%}\")\n",
    "print(f\"  Range: {few_shot_summary['min']:.2%} - {few_shot_summary['max']:.2%} (±{few_shot_summary['range']/2:.2%})\")\n",
    "print(f\"  Std Dev: {few_shot_summary['std_dev']:.4f}\")\n",
    "\n",
    "print(\"\\nZero-shot Self-Consistency CoT:\")\n",
    "print(f\"  Mean: {zero_shot_sc_summary['mean']:.2%}\")\n",
    "print(f\"  Range: {zero_shot_sc_summary['min']:.2%} - {zero_shot_sc_summary['max']:.2%} (±{zero_shot_sc_summary['range']/2:.2%})\")\n",
    "print(f\"  Std Dev: {zero_shot_sc_summary['std_dev']:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nFew-shot Self-Consistency CoT:\")\n",
    "print(f\"  Mean: {few_shot_sc_summary['mean']:.2%}\")\n",
    "print(f\"  Range: {few_shot_sc_summary['min']:.2%} - {few_shot_sc_summary['max']:.2%} (±{few_shot_sc_summary['range']/2:.2%})\")\n",
    "print(f\"  Std Dev: {few_shot_sc_summary['std_dev']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d8972",
   "metadata": {},
   "source": [
    "# PART III: Analysis and Discussions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a92f4",
   "metadata": {},
   "source": [
    "**[Q3.1]** Based on the metrics result file you obtatined from the evaluations, visualise the mean accuracies/solve rates of different CoT approaches in ONE bar chart for comparison using matplotlib (already imported). Please include error bars representing the standard deviation. (Use GenAI tools to help you generate the codes.)  **[5 Marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee934d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your visualisation codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa09e5",
   "metadata": {},
   "source": [
    "**[Q3.2]**: Based on the evaluation results, Which CoT approach performed best? Please analyse the EVALUATION SUMMARY results and summarise the findings.  **[6 Marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af11be",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6f2eea",
   "metadata": {},
   "source": [
    "**[Q3.3]** Considering the available parameters in the `eval_config`, which parameters do you think can be adjusted to further enhance the performance of (Zero-shot/Few-shot) SC-CoT? Please provide your reasoning. (You can get some ideas by observing the saved responses or the responses generated in the test eveluation for SC-CoT in Q1.10. You do not need to verify your speculations in this project.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce8b7b",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
